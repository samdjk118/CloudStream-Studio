from fastapi import APIRouter, BackgroundTasks, HTTPException
from models import MergeRequest, TaskResponse, VideoMetadata, RenameVideoRequest, SearchVideosRequest, SearchVideosResponse, ClipWithNameRequest
from services.gcs_service import GCSService
from services.ffmpeg_service import FFmpegService
from services.hls_service import HLSService
from utils.task_manager import task_manager
from config import get_settings
import tempfile
import os
import shutil
import logging
import subprocess
from typing import Optional, List
from datetime import datetime

router = APIRouter(prefix="/api/videos", tags=["Video Processing"])
logger = logging.getLogger(__name__)
settings = get_settings()

gcs_service = GCSService()
ffmpeg_service = FFmpegService()
hls_service = HLSService()

# ==================== è¼”åŠ©å‡½æ•¸ ====================

def get_video_metadata_from_gcs(blob) -> VideoMetadata:
    """å¾ GCS blob æå–å½±ç‰‡å…ƒæ•¸æ“š"""
    try:
        blob.reload()
        metadata = blob.metadata or {}
        
        # å–å¾—é¡¯ç¤ºåç¨±
        display_name = metadata.get('display_name')
        if not display_name:
            display_name = metadata.get('original_name', blob.name.split('/')[-1])
            # ç§»é™¤å‰¯æª”å
            if display_name.endswith('.mp4'):
                display_name = display_name[:-4]
        
        # å–å¾—å½±ç‰‡ ID (ä½¿ç”¨ GCS è·¯å¾‘ä½œç‚ºå”¯ä¸€ ID)
        video_id = blob.name
        
        # è§£æå½±ç‰‡è³‡è¨Š
        duration = None
        width = None
        height = None
        codec = None
        fps = None
        
        if 'duration' in metadata:
            try:
                duration = float(metadata['duration'])
            except (ValueError, TypeError):
                pass
        
        if 'width' in metadata:
            try:
                width = int(metadata['width'])
            except (ValueError, TypeError):
                pass
        
        if 'height' in metadata:
            try:
                height = int(metadata['height'])
            except (ValueError, TypeError):
                pass
        
        codec = metadata.get('codec')
        
        if 'fps' in metadata:
            try:
                fps = float(metadata['fps'])
            except (ValueError, TypeError):
                pass
        
        # å–å¾—ç¸®åœ– URL
        thumbnail_url = metadata.get('thumbnail_url')
        
        # ç”Ÿæˆä¸²æµ URL
        stream_url = f"/api/stream/{blob.name}"
        
        return VideoMetadata(
            id=video_id,
            original_name=metadata.get('original_name', blob.name.split('/')[-1]),
            display_name=display_name,
            gcs_path=blob.name,
            size=blob.size,
            duration=duration,
            width=width,
            height=height,
            codec=codec,
            fps=fps,
            upload_time=blob.time_created or datetime.now(),
            thumbnail_url=thumbnail_url,
            stream_url=stream_url
        )
    except Exception as e:
        logger.error(f"âŒ è§£æå½±ç‰‡å…ƒæ•¸æ“šå¤±æ•—: {e}", exc_info=True)
        raise

# ==================== å½±ç‰‡åˆ—è¡¨èˆ‡æœå°‹ ====================

@router.get("/list", response_model=List[VideoMetadata])
async def list_videos(
    search: Optional[str] = None,
    limit: int = 100,
    include_clips: bool = False
):
    """
    åˆ—å‡ºæ‰€æœ‰å½±ç‰‡
    
    Args:
        search: æœå°‹é—œéµå­—ï¼ˆå¯é¸ï¼‰
        limit: æœ€å¤§çµæœæ•¸ï¼ˆé è¨­ 100ï¼‰
        include_clips: æ˜¯å¦åŒ…å«å‰ªè¼¯ç‰‡æ®µï¼ˆé è¨­ Falseï¼‰
    """
    try:
        logger.info(f"ğŸ“‹ åˆ—å‡ºå½±ç‰‡ (æœå°‹: {search or 'ç„¡'}, é™åˆ¶: {limit})")
        
        bucket = gcs_service.storage_client.bucket(settings.GCS_BUCKET_NAME)
        blobs = bucket.list_blobs()
        
        videos = []
        for blob in blobs:
            # éæ¿¾æ¢ä»¶
            if not blob.name.endswith('.mp4'):
                continue
            
            # æ˜¯å¦åŒ…å«å‰ªè¼¯ç‰‡æ®µ
            if not include_clips:
                if '/clips/' in blob.name or '/merged/' in blob.name:
                    continue
            
            try:
                video_data = get_video_metadata_from_gcs(blob)
                
                # æœå°‹éæ¿¾
                if search:
                    search_lower = search.lower()
                    if not (search_lower in video_data.display_name.lower() or 
                           search_lower in video_data.original_name.lower()):
                        continue
                
                videos.append(video_data)
                
                # é™åˆ¶çµæœæ•¸
                if len(videos) >= limit:
                    break
                    
            except Exception as e:
                logger.warning(f"âš ï¸  è·³éç„¡æ•ˆå½±ç‰‡ {blob.name}: {e}")
                continue
        
        # æŒ‰ä¸Šå‚³æ™‚é–“æ’åºï¼ˆæœ€æ–°åœ¨å‰ï¼‰
        videos.sort(key=lambda x: x.upload_time, reverse=True)
        
        logger.info(f"   âœ… æ‰¾åˆ° {len(videos)} å€‹å½±ç‰‡")
        return videos
        
    except Exception as e:
        logger.error(f"âŒ åˆ—å‡ºå½±ç‰‡å¤±æ•—: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"åˆ—å‡ºå½±ç‰‡å¤±æ•—: {str(e)}")


@router.post("/search", response_model=SearchVideosResponse)
async def search_videos(request: SearchVideosRequest):
    """
    æœå°‹å½±ç‰‡
    
    Args:
        request: æœå°‹è«‹æ±‚ï¼ˆåŒ…å« query å’Œ limitï¼‰
    """
    try:
        logger.info(f"ğŸ” æœå°‹å½±ç‰‡: {request.query}")
        
        bucket = gcs_service.storage_client.bucket(settings.GCS_BUCKET_NAME)
        blobs = bucket.list_blobs()
        
        videos = []
        query_lower = request.query.lower()
        
        for blob in blobs:
            if not blob.name.endswith('.mp4'):
                continue
            
            # æ’é™¤å‰ªè¼¯å’Œåˆä½µç‰‡æ®µ
            if '/clips/' in blob.name or '/merged/' in blob.name:
                continue
            
            try:
                video_data = get_video_metadata_from_gcs(blob)
                
                # æœå°‹ display_name å’Œ original_name
                if (query_lower in video_data.display_name.lower() or 
                    query_lower in video_data.original_name.lower()):
                    videos.append(video_data)
                
                # é™åˆ¶çµæœæ•¸
                if len(videos) >= request.limit:
                    break
                    
            except Exception as e:
                logger.warning(f"âš ï¸  è·³éç„¡æ•ˆå½±ç‰‡ {blob.name}: {e}")
                continue
        
        # æŒ‰ä¸Šå‚³æ™‚é–“æ’åº
        videos.sort(key=lambda x: x.upload_time, reverse=True)
        
        logger.info(f"   âœ… æ‰¾åˆ° {len(videos)} å€‹çµæœ")
        
        return SearchVideosResponse(
            videos=videos,
            total=len(videos),
            query=request.query
        )
        
    except Exception as e:
        logger.error(f"âŒ æœå°‹å¤±æ•—: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æœå°‹å¤±æ•—: {str(e)}")


# ==================== é‡æ–°å‘½å ====================

@router.put("/rename", response_model=VideoMetadata)
async def rename_video(request: RenameVideoRequest):
    """
    é‡æ–°å‘½åå½±ç‰‡
    
    Args:
        request: åŒ…å« gcs_path å’Œ new_name
    """
    try:
        logger.info(f"âœï¸ é‡æ–°å‘½åå½±ç‰‡: {request.gcs_path} -> {request.new_name}")
        
        # æª¢æŸ¥å½±ç‰‡æ˜¯å¦å­˜åœ¨
        if not gcs_service.file_exists(request.gcs_path):
            raise HTTPException(status_code=404, detail="å½±ç‰‡ä¸å­˜åœ¨")
        
        # å–å¾— blob
        bucket = gcs_service.storage_client.bucket(settings.GCS_BUCKET_NAME)
        blob = bucket.blob(request.gcs_path)
        blob.reload()
        
        # æ›´æ–° metadata
        current_metadata = blob.metadata or {}
        current_metadata['display_name'] = request.new_name
        blob.metadata = current_metadata
        blob.patch()
        
        logger.info(f"   âœ… é‡æ–°å‘½åæˆåŠŸ")
        
        # æ¸…é™¤å¿«å–
        try:
            from services.gcs_cache import get_connection_pool
            gcs_pool = get_connection_pool()
            gcs_pool.invalidate_metadata_cache(settings.GCS_BUCKET_NAME, request.gcs_path)
            logger.info(f"   âœ… å·²æ¸…é™¤ metadata å¿«å–")
        except Exception as e:
            logger.warning(f"   âš ï¸  æ¸…é™¤å¿«å–å¤±æ•—: {e}")
        
        # è¿”å›æ›´æ–°å¾Œçš„è³‡æ–™
        return get_video_metadata_from_gcs(blob)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ é‡æ–°å‘½åå¤±æ•—: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"é‡æ–°å‘½åå¤±æ•—: {str(e)}")


# ==================== åˆä½µå¤šå€‹ç‰‡æ®µ ====================
@router.post("/merge", response_model=TaskResponse)
async def merge_videos(request: MergeRequest, background_tasks: BackgroundTasks):
    """
    åˆä½µå¤šå€‹å½±ç‰‡ç‰‡æ®µï¼ˆæ¯«ç§’ç´šç²¾åº¦ï¼‰
    
    - è‡ªå‹•è™•ç†ä¸åŒæ ¼å¼å’Œç·¨ç¢¼çš„å½±ç‰‡
    - æ”¯æŒæ¯«ç§’ç´šæ™‚é–“ç²¾åº¦
    """
    if len(request.clips) < 1:
        raise HTTPException(status_code=400, detail="At least one clip is required")
    
    task_id = task_manager.create_task(
        f"Merge task created ({len(request.clips)} clips)"
    )
    
    background_tasks.add_task(
        process_merge_task,
        task_id,
        request
    )
    
    return TaskResponse(
        task_id=task_id,
        message=f"Merge task started with {len(request.clips)} clips",
        status_url=f"/api/tasks/{task_id}"
    )


async def process_merge_task(task_id: str, request: MergeRequest):
    """åŸ·è¡Œåˆä½µä»»å‹™ï¼ˆæ¯«ç§’ç´šç²¾åº¦ï¼‰"""
    temp_dir = tempfile.mkdtemp(prefix="merge_")
    clip_files = []
    clip_durations = []
    
    try:
        task_manager.update_task(
            task_id,
            status="processing",
            progress=0.1,
            message="Processing clips with millisecond precision..."
        )
        
        total_clips = len(request.clips)
        logger.info(f"ğŸ”— [Task {task_id}] é–‹å§‹åˆä½µ {total_clips} å€‹ç‰‡æ®µï¼ˆæ¯«ç§’ç´šç²¾åº¦ï¼‰")
        
        # âœ… è¨ˆç®—é æœŸç¸½æ™‚é•·
        expected_total_duration = 0.0
        for clip in request.clips:
            clip_duration = round(clip.end_time - clip.start_time, 3)
            expected_total_duration += clip_duration
            logger.info(f"   ç‰‡æ®µ: {clip.source_video}")
            logger.info(f"      ç¯„åœ: {clip.start_time:.3f}s - {clip.end_time:.3f}s")
            logger.info(f"      æ™‚é•·: {clip_duration:.3f}s ({int(clip_duration * 1000)}ms)")
        
        expected_total_duration = round(expected_total_duration, 3)
        logger.info(f"   é æœŸç¸½æ™‚é•·: {expected_total_duration:.3f}s ({int(expected_total_duration * 1000)}ms)")
        
        # ==================== 1. è™•ç†æ¯å€‹ç‰‡æ®µ ====================
        for i, clip in enumerate(request.clips):
            logger.info(f"   è™•ç†ç‰‡æ®µ {i+1}/{total_clips}: {clip.source_video}")
            
            # ä¸‹è¼‰åŸå§‹å½±ç‰‡
            local_input = os.path.join(temp_dir, f"input_{i}.mp4")
            gcs_service.download_file(clip.source_video, local_input)
            
            # ç²å–å½±ç‰‡ä¿¡æ¯
            video_info = ffmpeg_service.get_video_info(local_input)
            logger.info(f"      åŸå§‹æ™‚é•·: {video_info['duration']:.3f}s, åˆ†è¾¨ç‡: {video_info['width']}x{video_info['height']}")
            
            # âœ… å‰ªè¼¯ç‰‡æ®µï¼ˆæ¯«ç§’ç´šç²¾åº¦ï¼‰
            clip_output = os.path.join(temp_dir, f"clip_{i:03d}.mp4")
            logger.info(f"      å‰ªè¼¯: {clip.start_time:.3f}s - {clip.end_time:.3f}s")
            
            ffmpeg_service.clip_video(
                local_input,
                clip_output,
                clip.start_time,
                clip.end_time,
                re_encode=True,  # âœ… åˆä½µæ™‚éœ€è¦é‡æ–°ç·¨ç¢¼ä»¥ç¢ºä¿å…¼å®¹æ€§
                precise=True     # âœ… æ¯«ç§’ç´šç²¾åº¦
            )
            
            # é©—è­‰å‰ªè¼¯çµæœ
            clip_info = ffmpeg_service.get_video_info(clip_output)
            actual_clip_duration = round(clip_info['duration'], 3)
            expected_clip_duration = round(clip.end_time - clip.start_time, 3)
            
            logger.info(f"      å‰ªè¼¯å¾Œæ™‚é•·: {actual_clip_duration:.3f}s")
            logger.info(f"      é æœŸæ™‚é•·: {expected_clip_duration:.3f}s")
            
            clip_error = abs(actual_clip_duration - expected_clip_duration)
            clip_error_ms = int(clip_error * 1000)
            logger.info(f"      èª¤å·®: {clip_error:.3f}s ({clip_error_ms}ms)")
            
            clip_files.append(clip_output)
            clip_durations.append(actual_clip_duration)
            
            # æ›´æ–°é€²åº¦
            progress = 0.1 + (0.6 * (i + 1) / total_clips)
            task_manager.update_task(
                task_id,
                progress=progress,
                message=f"Processed clip {i+1}/{total_clips} ({actual_clip_duration:.3f}s)"
            )
            
            # æ¸…ç†è¼¸å…¥æ–‡ä»¶
            os.remove(local_input)
        
        # ==================== 2. åˆä½µå½±ç‰‡ ====================
        task_manager.update_task(task_id, progress=0.7, message="Merging clips...")
        
        logger.info(f"ğŸ”— [Task {task_id}] åˆä½µæ‰€æœ‰ç‰‡æ®µ...")
        merged_output = os.path.join(temp_dir, "merged.mp4")
        
        # âœ… ä½¿ç”¨é‡æ–°ç·¨ç¢¼æ¨¡å¼ä»¥ç¢ºä¿ç²¾åº¦
        ffmpeg_service.merge_videos(
            clip_files, 
            merged_output, 
            re_encode=True  # é‡æ–°ç·¨ç¢¼ä»¥ç¢ºä¿å…¼å®¹æ€§å’Œç²¾åº¦
        )
        
        # âœ… é©—è­‰åˆä½µçµæœï¼ˆæ¯«ç§’ç´šç²¾åº¦ï¼‰
        merged_info = ffmpeg_service.get_video_info(merged_output)
        actual_total_duration = round(merged_info['duration'], 3)
        
        logger.info(f"   âœ… åˆä½µå®Œæˆ")
        logger.info(f"   å¯¦éš›ç¸½æ™‚é•·: {actual_total_duration:.3f}s ({int(actual_total_duration * 1000)}ms)")
        logger.info(f"   é æœŸç¸½æ™‚é•·: {expected_total_duration:.3f}s ({int(expected_total_duration * 1000)}ms)")
        
        # âœ… è¨ˆç®—ç¸½èª¤å·®
        total_error = abs(actual_total_duration - expected_total_duration)
        total_error_ms = int(total_error * 1000)
        total_error_percent = (total_error / expected_total_duration) * 100 if expected_total_duration > 0 else 0
        
        logger.info(f"   èª¤å·®: {total_error:.3f}s ({total_error_ms}ms, {total_error_percent:.2f}%)")
        
        # âœ… ç²¾åº¦è©•ä¼°
        if total_error < 0.050:
            logger.info(f"   âœ… åˆä½µç²¾åº¦ï¼šå„ªç§€ (< 50ms)")
            merge_precision = "excellent"
        elif total_error < 0.100:
            logger.info(f"   âœ“ åˆä½µç²¾åº¦ï¼šè‰¯å¥½ (< 100ms)")
            merge_precision = "good"
        elif total_error < 0.200:
            logger.info(f"   â—‹ åˆä½µç²¾åº¦ï¼šå¯æ¥å— (< 200ms)")
            merge_precision = "acceptable"
        else:
            logger.warning(f"   âš ï¸  åˆä½µç²¾åº¦ï¼šä¸€èˆ¬ (> 200ms)")
            merge_precision = "fair"
        
        # ==================== 3. ä¸Šå‚³åˆ° GCS ====================
        task_manager.update_task(task_id, progress=0.9, message="Uploading result...")
        output_name = request.output_name+".mp4"
        output_path = f"merged/{output_name}"
        logger.info(f"ğŸ“¤ [Task {task_id}] ä¸Šå‚³åˆ° GCS: {output_path}")
        gcs_service.upload_file(merged_output, output_path)
        
        # è¨­ç½® metadataï¼ˆåŒ…å« display_nameï¼‰
        bucket = gcs_service.client.bucket(settings.GCS_BUCKET_NAME)
        blob = bucket.blob(output_path)
        
        # ç§»é™¤ .mp4 å‰¯æª”åä½œç‚ºé¡¯ç¤ºåç¨±
        display_name = output_name[:-4] if output_name.endswith('.mp4') else output_name
        
        blob.metadata = {
            'original_name': request.output_name,
            'display_name': display_name,  # è¨­ç½®é¡¯ç¤ºåç¨±
            'duration': str(actual_total_duration),
            'width': str(merged_info['width']),
            'height': str(merged_info['height']),
            'codec': merged_info['codec'],
            'fps': str(merged_info['fps']),
            'total_clips': str(total_clips),
            'created_by': 'merge_task'
        }
        blob.patch()
        
        logger.info(f"   å·²è¨­ç½® metadata: display_name = {display_name}")
        # ==================== 4. ç”Ÿæˆç¸®åœ– ====================
        thumbnail_local = os.path.join(temp_dir, "thumbnail.jpg")
        thumbnail_time = round(actual_total_duration / 2, 3)
        ffmpeg_service.generate_thumbnail(
            merged_output, 
            thumbnail_local,
            timestamp=thumbnail_time
        )
        
        thumbnail_path = f"thumbnails/{output_name}.jpg"
        gcs_service.upload_file(thumbnail_local, thumbnail_path)
        
        # âœ… æ›´æ–° metadata åŠ å…¥ç¸®åœ– URL
        blob.metadata['thumbnail_url'] = gcs_service.get_public_url(thumbnail_path)
        blob.patch()

        # ==================== 5. å®Œæˆ ====================
        output_url = gcs_service.get_public_url(output_path)
        thumbnail_url = gcs_service.get_public_url(thumbnail_path)
        
        # âœ… è¿”å›æ¯«ç§’ç´šç²¾åº¦çš„ metadata
        task_manager.update_task(
            task_id,
            status="completed",
            progress=1.0,
            message="Merge completed successfully with millisecond precision",
            output_url=output_url,
            output_path=output_path,
            metadata={
                "output_name": request.output_name,  
                "display_name": display_name,       
                "total_clips": total_clips,
                "merged_duration": actual_total_duration,
                "expected_duration": expected_total_duration,
                "duration_error_ms": total_error_ms,
                "duration_error_percent": round(total_error_percent, 2),
                "precision_level": merge_precision,
                "clip_durations": clip_durations,
                "file_size": os.path.getsize(merged_output),
                "thumbnail_url": thumbnail_url,
                "video_info": {
                    "width": merged_info['width'],
                    "height": merged_info['height'],
                    "codec": merged_info['codec'],
                    "fps": merged_info['fps']
                }
            }
        )
        
        logger.info(f"âœ… [Task {task_id}] åˆä½µä»»å‹™å®Œæˆï¼ˆæ¯«ç§’ç´šç²¾åº¦ï¼‰")
        logger.info(f"   è¼¸å‡º URL: {output_url}")
        logger.info(f"   ç²¾åº¦ç­‰ç´š: {merge_precision}")
        
    except Exception as e:
        logger.error(f"âŒ [Task {task_id}] åˆä½µä»»å‹™å¤±æ•—: {e}", exc_info=True)
        task_manager.update_task(
            task_id,
            status="failed",
            error=str(e),
            message=f"Merge failed: {str(e)}"
        )
    
    finally:
        shutil.rmtree(temp_dir, ignore_errors=True)

@router.post("/optimize/{video_path:path}")
async def optimize_video(video_path: str, background_tasks: BackgroundTasks):
    """
    æœ€ä½³åŒ–å½±ç‰‡ï¼ˆFast Startï¼‰
    
    å°‡ moov atom ç§»åˆ°æª”æ¡ˆé–‹é ­ï¼ŒåŠ é€Ÿä¸²æµè¼‰å…¥
    
    Args:
        video_path: GCS ä¸­çš„å½±ç‰‡è·¯å¾‘
    
    Returns:
        TaskResponse: ä»»å‹™è³‡è¨Š
    
    Example:
        ```bash
        curl -X POST http://localhost:8000/api/videos/optimize/uuid/video.mp4/timestamp/sample.mp4
        ```
    """
    try:
        if not gcs_service.file_exists(video_path):
            raise HTTPException(status_code=404, detail="å½±ç‰‡ä¸å­˜åœ¨")
        
        logger.info(f"ğŸ”§ æœ€ä½³åŒ–å½±ç‰‡: {video_path}")
        
        # å‰µå»ºä»»å‹™
        task_id = task_manager.create_task(f"æœ€ä½³åŒ–: {os.path.basename(video_path)}")
        
        # åœ¨èƒŒæ™¯åŸ·è¡Œ
        background_tasks.add_task(
            process_optimize_task,
            task_id,
            video_path
        )
        
        return TaskResponse(
            task_id=task_id,
            message="æœ€ä½³åŒ–ä»»å‹™å·²å•Ÿå‹•",
            status_url=f"/api/tasks/{task_id}"
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ å‰µå»ºæœ€ä½³åŒ–ä»»å‹™å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"å‰µå»ºä»»å‹™å¤±æ•—: {str(e)}")


async def process_optimize_task(task_id: str, video_path: str):
    """åŸ·è¡Œæœ€ä½³åŒ–ä»»å‹™"""
    temp_dir = None
    try:
        task_manager.update_task(
            task_id,
            status="processing",
            progress=0.1,
            message="ä¸‹è¼‰å½±ç‰‡..."
        )
        
        logger.info(f"ğŸ”§ [Task {task_id}] é–‹å§‹æœ€ä½³åŒ–: {video_path}")
        
        temp_dir = tempfile.mkdtemp(prefix="optimize_")
        local_input = os.path.join(temp_dir, "input.mp4")
        local_output = os.path.join(temp_dir, "output.mp4")
        
        # ä¸‹è¼‰å½±ç‰‡
        gcs_service.download_file(video_path, local_input)
        
        task_manager.update_task(
            task_id,
            progress=0.3,
            message="åŸ·è¡Œ Fast Start æœ€ä½³åŒ–..."
        )
        
        # ä½¿ç”¨ ffmpeg é€²è¡Œ Fast Start æœ€ä½³åŒ–
        ffmpeg_cmd = [
            'ffmpeg',
            '-i', local_input,
            '-c', 'copy',  # ä¸é‡æ–°ç·¨ç¢¼
            '-movflags', '+faststart',  # Fast Start
            '-y',
            local_output
        ]
        
        logger.info(f"   åŸ·è¡Œ: {' '.join(ffmpeg_cmd)}")
        
        process = subprocess.run(
            ffmpeg_cmd,
            capture_output=True,
            text=True,
            timeout=300
        )
        
        if process.returncode != 0:
            raise Exception(f"FFmpeg å¤±æ•—: {process.stderr}")
        
        task_manager.update_task(
            task_id,
            progress=0.7,
            message="ä¸Šå‚³æœ€ä½³åŒ–ç‰ˆæœ¬..."
        )
        
        # âœ… ä¸Šå‚³å› GCSï¼ˆè¦†è“‹åŸæª”æ¡ˆï¼‰
        gcs_service.upload_file(local_output, video_path)
        
        # âœ… æ¸…é™¤ GCS metadata å¿«å–
        from services.gcs_cache import get_connection_pool
        gcs_pool = get_connection_pool()
        gcs_pool.invalidate_metadata_cache(settings.GCS_BUCKET_NAME, video_path)
        logger.info(f"   âœ… å·²æ¸…é™¤ metadata å¿«å–: {video_path}")
        
        # âœ… æ¸…é™¤å½±ç‰‡å¿«å–
        from services.video_cache import get_video_cache
        video_cache = get_video_cache()
        video_cache.invalidate(video_path)
        logger.info(f"   âœ… å·²æ¸…é™¤å½±ç‰‡å¿«å–: {video_path}")
        
        # ç²å–æª”æ¡ˆè³‡è¨Š
        optimized_info = ffmpeg_service.get_video_info(local_output)
        
        task_manager.update_task(
            task_id,
            status="completed",
            progress=1.0,
            message="æœ€ä½³åŒ–å®Œæˆ",
            output_path=video_path,
            metadata={
                "optimized": True,
                "file_size": os.path.getsize(local_output),
                "duration": optimized_info['duration'],
                "video_info": {
                    "width": optimized_info['width'],
                    "height": optimized_info['height'],
                    "codec": optimized_info['codec'],
                    "fps": optimized_info['fps']
                }
            }
        )
        
        logger.info(f"âœ… [Task {task_id}] æœ€ä½³åŒ–å®Œæˆ")
        
    except Exception as e:
        logger.error(f"âŒ [Task {task_id}] æœ€ä½³åŒ–å¤±æ•—: {e}", exc_info=True)
        task_manager.update_task(
            task_id,
            status="failed",
            error=str(e),
            message=f"æœ€ä½³åŒ–å¤±æ•—: {str(e)}"
        )
    finally:
        if temp_dir and os.path.exists(temp_dir):
            shutil.rmtree(temp_dir, ignore_errors=True)